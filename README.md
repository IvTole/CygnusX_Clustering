# CygnusX_Clustering

We use data from the Nobeyama 45m radiotelescope towards CygnusX star forming region. The objective is to apply a clustering technique (python package astrodendro) to detect c18o clumps in the region.

## Repository Structure
```
.
├── data/         # Original data cubes in .fits format (for 12co, 13co and c18o (J=1-0)).
├── fitsfiles/    # Output fits files from the script. At the moment, this includes mom8 and smoothed cubes files.
├── modules/      # Python scripts for running the program and modules. Main script : run_clustering.py
├── plots/        # Output plots from the script.
├── catalog/      # Output catalog (csv) from the script. This includes catalogs from astrodendro, fit parameters from Gaussian fit, and final catalog of clumps with physical parameters.
├── mask/         # Output mask (numpy array) from the script, where clusters are identified.
├── environment/  # Conda environment for python.
└── README.md     # Repository documentation

```

## Conda environment setup

Inside directory `environment/` there is a file named `environment.yml`. This file is used to set up a dedicated Conda environment with all the necessary dependencies for running the code in this repository.

To create the environment, first ensure you have **Anaconda** or **Miniconda** installed on your system. You can download it from [Anaconda's official website](https://www.anaconda.com/download). Then, open a terminal and run the following command:


```bash
conda env create -f environment.yml
```

This command will create a new Conda environment named `astrophysics`, installing all required libraries and dependencies automatically.

#### Activating and Deactivating the Environment

Once the installation is complete, you can activate the new environment by running:


```bash
conda activate astrophysics
```

If you need to switch to another environment or deactivate it, simpy run:

```bash
conda deactivate
```

## Running the Clustering Script

The `modules/` directory contains the main script **`run_clustering.py`**, which serves as the primary entry point for executing the clustering process. To run the script, simply use the following command in the terminal (with activated conda environment):  

```bash
python run_clustering.py
```

#### Configuring execution stages

At the beginning of `run_clustering.py`, ther is a specific line that defines which stages of teh clustering precess will be executed:

## Running the Clustering Script

The `modules/` directory contains the main script **`run_clustering.py`**, which serves as the primary entry point for executing the clustering process. To run the script, simply use the following command in the terminal:  

```python
stages = [1]
```

This variable controls which steps of the pipeline will run. It is recommended to preceed step by step, modifying this line, and then incrementing the stage number in subsequent runs. Each stage generates new output files in the repository directories.

## Test data
The script utilizes test data from the DR21 regions, as described in **Takekoshi et al. (2019) - Nobeyama Cygnus X Survey**

## Output files
Each stage of execution produces specific output files, shich will be stored in the appropriate directories within the repository. The exact files generated by each stage are deteiled in the following sections.

## Stage 1: Data Smoothing and Moment 8 Calculation

In this stage, the script processes the input data cubes by applying a **smoothing procedure**. By default, the smoothing factor is set to **1**, but this can be adjusted if needed.

### Outputs Generated:

1. **Smoothed Data Cubes:**  
   - The script creates new **FITS** files with smoothed data (cubes) and saves them in the `fits/` directory.  

2. **Moment 8 FITS Files:**  
   - New FITS files containing the **Moment 8** maps (2d) are generated and stored in `fits/`.  

3. **Moment 8 Plots:**  
   - The script produces **PDF plots** of the Moment 8 maps, which are saved in the `plots/` directory.  

### Modifiable Parameters

The following parameters can be adjusted to customize the processing and visualization in **Stage 1**:

- **`efficiency=1.0`** → Main beam efficiency correction (applied if available).  
- **`kernel_px=1`** → Kernel size for the smoothing process (in pixels).  
- **`gamma=1.0`** → Gamma factor applied during moment plotting.  
- **`vmin=0.0`**, **`vmax=45.0`** → Minimum and maximum intensity values for moment visualization.

## Stage 2: Clustering Process

In this stage, the script applies a **clustering algorithm** to the smoothed data cubes. The clustering is performed using the **Astrodendro** package, and the results are printed in the terminal.

### Outputs Generated:

1. **Clustering Execution & Terminal Output:**  
   - The clustering process is executed, and relevant information about the identified structures is printed in the terminal.

2. **Initial Catalog (`catalog/`):**  
   - A **preliminary catalog** of detected structures is generated and saved as:  
     ```
     catalog/dr21_catalog_c18o.csv
     ```
     - This catalog contains the detected clusters' properties **in pixel units**.

3. **Cluster Masks (`mask/`):**  
   - A NumPy file containing the **identified cluster masks** (in 2d, maximum number of pixels projected in the sky) is created and stored as:  
     ```
     mask/dr21_c18o_masks.npy
     ```

4. **Clustering Visualization (`plots/`):**  
   - A **PDF plot** is generated with the **Astrodendro contour map** overlaying the maximum intensity projection:  
     ```
     plots/dr21_c18o_max_astrodendro_contours.pdf
     ```
   - This visualization helps verify whether the **identified clusters** align with expectations.

### Astrodendro Hyperparameters

The following hyperparameters (stage 2) are used for the **Astrodendro** clustering process and can by modified:

- **`T_rms = 0.35 K`** → Root Mean Square (RMS) noise level, corrected for main beam efficiency.  
- **`T_min = 3.0 * T_rms`** → Minimum intensity threshold for structure detection.  
- **`T_delta = 2.0 * T_rms`** → Minimum intensity difference required between hierarchical structures.  
- **`n_vox = 16`** → Minimum number of connected voxels required for a structure to be considered significant.  

These parameters control the sensitivity and granularity of the clustering algorithm, affecting the number and size of the detected structures.


### Important Note:
At this stage, it is **crucial** to check the generated plot (`dr21_c18o_max_astrodendro_contours.pdf`) to ensure that the clustering is identifying **valid** and **meaningful** structures in the data.

## Stage 3: Manual Cluster Refinement

In this stage, the **initial catalog and mask** from Stage 2 are used to generate reference plots. These plots help visually inspect and **manually remove unwanted clusters** by modifying the `drop_list` within the `stage3` function.

### Process:

1. **Reference Plots (`plots/`):**  
   - Before removing clusters:  
     ```
     plots/dr21_c18o_catalog_not_dropped.pdf
     ```
   - After manual refinement:  
     ```
     plots/dr21_c18o_catalog_dropped.pdf
     ```

2. **Manual Cluster Removal:**
   - Unwanted clusters are **removed by adding their IDs to the `drop_list`** in `stage3`.
   - The function **automatically updates the catalog and mask** based on this list.

3. **Updated Mask & Catalog (`catalog/` and `mask/`):**  
   - The **refined catalog** and **mask files** are saved after filtering out undesired clusters (prefix _not_dropped_ and _dropped_).

### Important Note:
- The `drop_list` must be manually edited before running this stage.
- The updated catalog and mask will be used in subsequent processing stages.

## Stage 4: Extracting, Fitting Spectra, and Storing Parameters

In this stage, the **refined catalog and mask** from Stage 3 are used to extract spectra from the **original (unsmoothed) data cubes**, which may not be corrected for **main beam efficiency**. The **efficiency parameter** can be adjusted (default is `efficiency = 1.0`) during the spectra extraction process.

Additionally, a function has been included to **automatically detect peaks** in the spectra using the `scipy.signal.find_peaks` function. The peaks are distinguished using the parameters `height` and `distance`. The `height` parameter is used to specify the minimum height of a peak, while `distance` parameter is used to specify the minimum number of data points between consecutive peaks. This helps prevent the function from detecting multiple peaks that are too close to each other, which might be due to noise or very fine structures in the signal.

### Process:

1. **Extract Spectra:**  
   - Spectra are extracted from the **original data cubes**, with the option to adjust the **main beam efficiency correction** using the `efficiency` parameter.

2. **Peak Detection:**  
   - The `find_peaks` function identifies peaks in the spectra, requiring the parameters:
     - `height`: minimum height of a peak
     - `distance`: minimum distance between peaks to distinguish them
   - The number of detected peaks determines the number of Gaussians needed for fitting the spectra.

3. **Gaussian Fitting:**  
   - The number of detected peaks guides the fitting process, where each peak is modeled using a **Gaussian fit** with the following initial parameters:
     - **Height** of the peak
     - **Center** of the peak (detected by the peak finding function)
     - **Sigma** (standard deviation) is handled by the external module `module_data_spectra.py`
   - The **lmfit package** is used for Gaussian fitting.

4. **Storing Fit Parameters:**  
   - The fit parameters (height, center, sigma, etc.) for each detected Gaussian are stored in the **catalog folder** as CSV files:
     - `dr21_12co_fit_params.csv`
     - `dr21_13co_fit_params.csv`
     - `dr21_c18o_fit_params.csv`
   - These CSV files contain the fit parameters for each **Clump ID**, along with the parameters for each Gaussian used to fit the spectra.

### Important Notes:
- The efficiency parameter can be adjusted to account for main beam efficiency.
- The fitting procedure ensures the appropriate number of Gaussians is used based on the detected peaks in each spectrum.
- The **fit parameters** are saved for future reference and analysis.
- If any "clumps" have no signa, it may be necessary to drop this cluster in stage 3.

### Stage 5

In this stage, we analyze the spectra for each clump. Typically, the **C18O** spectra will only be fitted with a **single Gaussian** (representing the clump signal), but the **12CO** and **13CO** spectra may contain multiple spectral components. 

We proceed with the following steps:

1. **Component Selection**:
   - The closest spectral component (in LSR velocity) is chosen from the 12CO and 13CO spectra for fitting (the external module takes care of that).
   
2. **Physical Parameters Calculation**:
   - The physical parameters of the clump are calculated using the equations described in **Takekoshi et al. 2019**. 
   - However, there are two main differences in our approach:
     - Instead of using the **voxels** from the **Astrodendro**-identified regions, we use the **area of the adjusted mean spectra** derived from the **projected pixels** in the sky of the mask.
     - We do not apply the **deconvolution** of the signal using a square function, as suggested by Takekoshi et al. 2019.
   
3. **Catalog Creation**:
   - The calculated physical parameters are stored in a **CSV file** located in the **`catalog/`** folder: `dr21_clumps_catalog.csv`.
   - Additionally, a **LaTeX-formatted** version of the catalog is created in the file `dr21_clumps_catalog_latex.txt`, for easier integration into LaTeX documents.

